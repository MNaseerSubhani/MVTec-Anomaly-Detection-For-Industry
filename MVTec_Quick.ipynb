{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"MVTec_Quick.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YBqL4W8Chxi9"},"source":["#Introduction"]},{"cell_type":"markdown","metadata":{"id":"5tVeHtQDhzed"},"source":["This notebook is a easy guide for training and testing of anomaly detection based on MVTec https://github.com/AdneneBoumessouer/MVTec-Anomaly-Detection.\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1IzBqUvTSTYzoa3gG3_gLh3QQKbylrQNf#scrollTo=5tVeHtQDhzed)"]},{"cell_type":"code","metadata":{"id":"S_WPNR8qmHsq"},"source":["!pip3 install ktrain"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uESknW72h8u5"},"source":["###Link Google Drive"]},{"cell_type":"code","metadata":{"id":"sUcdjkC3hHYE"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxo8GmWVcM61"},"source":["%cd /content/drive/MyDrive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFW4bKv_0INH"},"source":["#Clone Repo"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2rc3zMU0MH7","executionInfo":{"status":"ok","timestamp":1628141850148,"user_tz":-300,"elapsed":1926,"user":{"displayName":"Muhammad Naseer Subhani","photoUrl":"","userId":"06699857406068047110"}},"outputId":"fcc2a011-576a-47d6-d922-72e0d715ff75"},"source":["!git clone https://github.com/MNaseerSubhani/MVTec-Anomaly-Detection-For-Industry.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'MVTec-Anomaly-Detection-For-Industry'...\n","remote: Enumerating objects: 45, done.\u001b[K\n","remote: Counting objects: 100% (45/45), done.\u001b[K\n","remote: Compressing objects: 100% (41/41), done.\u001b[K\n","remote: Total 45 (delta 8), reused 30 (delta 1), pack-reused 0\u001b[K\n","Unpacking objects: 100% (45/45), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E3nA-UGw0U4T"},"source":["%cd /content/drive/MyDrive/MVTec-Anomaly-Detection-For-Industry"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9AI5ZmnxtIA"},"source":["###Update Submodule : \n"]},{"cell_type":"code","metadata":{"id":"XbjzWP9zxkyY"},"source":["!git submodule init\n","!git submodule update"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MecSmAFxlX66"},"source":["#Dataset "]},{"cell_type":"markdown","metadata":{"id":"Z5ZvfjkglvRL"},"source":["### Directory Structure using your own dataset"]},{"cell_type":"markdown","metadata":{"id":"j1UD1k1Blj6E"},"source":["To train with your own dataset, you need to have a comparable directory structure. For example:"]},{"cell_type":"code","metadata":{"id":"08vGmfeplXv0"},"source":["#MVTec-Anomaly-Detection\n","      #data\n","        #├── class1\n","        # │   ├── test\n","        # │   │   ├── good\n","        # │   │   ├── defect\n","        # │   └── train\n","        # │       └── good\n","        # ├── class2\n","        # │   ├── test\n","        # │   │   ├── good\n","        # │   │   ├── defect\n","        # │   └── train\n","        # │       └── good\n","        # ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aacA9vucmQHq"},"source":["#Training "]},{"cell_type":"code","metadata":{"id":"8vn8bS_JnGwj"},"source":["# During training, the CAE trains exclusively on defect-free images and learns to reconstruct (predict) defect-free training samples.\n","\n","# usage: train.py [-h] -d [-a] [-c] [-l] [-b] [-i]\n","\n","# optional arguments:\n","\n","# -h, --help show this help message and exit\n","\n","# -d , --input-dir directory containing training images\n","\n","# -a , --architecture architecture of the model to use for training: 'mvtecCAE', 'baselineCAE', 'inceptionCAE' or 'resnetCAE'\n","\n","# -c , --color color mode for preprocessing images before training: 'rgb' or 'grayscale'\n","\n","# -l , --loss loss function to use for training: 'mssim', 'ssim' or 'l2'\n","\n","# -b , --batch batch size to use for training\n","\n","# -i, --inspect generate inspection plots after training\n","\n","# Example usage:\n","\n","# python3 train.py -d mvtec/capsule -a mvtecCAE -b 8 -l ssim -c grayscale\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7yvOxdedpRS"},"source":["!python3 ./MVTec-Anomaly-Detection/train.py -d data/class -a baselineCAE -b 16 -l l2 -c grayscale"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iXNG2EaPnOBF"},"source":["#Finetune "]},{"cell_type":"code","metadata":{"id":"LBKfp4uCjXDo"},"source":["# This script used a subset of defect-free training images and a subset of both defect and defect-free test images to determine good values for minimum defect area and threshold pair of parameters that will be used during testing for classification and segmentation.\n","\n","# usage: finetune.py [-h] -p [-m] [-t]\n","\n","# optional arguments: -h, --help show this help message and exit\n","\n","# -p , --path path to saved model\n","\n","# -m , --method method for generating resmaps: 'ssim' or 'l2'\n","\n","# -t , --dtype datatype for processing resmaps: 'float64' or 'uint8'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuFxIzVFsO_k"},"source":["!python3 ./MVTec-Anomaly-Detection/finetune.py -p path/to/model -m ssim -t float64"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRpMRL_AnTwU"},"source":["#Testing"]},{"cell_type":"code","metadata":{"id":"l5-ewa1RjXGX"},"source":["# This script classifies test images using the minimum defect area and threshold previously approximated at the finetuning step.\n","\n","# usage: test.py [-h] -p [-s]\n","\n","# optional arguments: -h, --help show this help message and exit\n","\n","# -p , --path path to saved model\n","\n","# -s, --save save segmented images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zZhWZ7Tsfhm"},"source":["!python3 ./MVTec-Anomaly-Detection/test.py -p path/to/model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuLdH362nXt2"},"source":["#Validate"]},{"cell_type":"code","metadata":{"id":"k-8m8Aj-jXJP"},"source":["%cd ./MVTec-Anomaly-Detection\n","from skimage.measure import compare_ssim\n","import sys\n","import os\n","import argparse\n","from pathlib import Path\n","import shlex\n","import time\n","import json\n","import tensorflow as tf\n","import seaborn as sns\n","import matplotlib.pyplot as plt     \n","from processing import utils\n","from processing import postprocessing\n","from processing.preprocessing import Preprocessor\n","from processing.preprocessing import get_preprocessing_function\n","from processing.postprocessing import label_images\n","from processing.utils import printProgressBar\n","from skimage.util import img_as_ubyte\n","from sklearn.metrics import confusion_matrix\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import glob\n","import cv2\n","import seaborn as sn\n","import seaborn as sns\n","import matplotlib.pyplot as plt    \n","import pandas as pd\n","import logging\n","from sklearn.metrics import confusion_matrix\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","%cd .."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQssvR4f0uTp"},"source":["def Test(args, th):\n","  model_path = args.path\n","  # load model and info\n","  model, info, _ = utils.load_model_HDF5(model_path)\n","  # set parameters\n","  input_directory = info[\"data\"][\"input_directory\"]\n","  architecture = info[\"model\"][\"architecture\"]\n","  loss = info[\"model\"][\"loss\"]\n","  rescale = info[\"preprocessing\"][\"rescale\"]\n","  shape = info[\"preprocessing\"][\"shape\"]\n","  color_mode = info[\"preprocessing\"][\"color_mode\"]\n","  vmin = info[\"preprocessing\"][\"vmin\"]\n","  vmax = info[\"preprocessing\"][\"vmax\"]\n","  nb_validation_images = info[\"data\"][\"nb_validation_images\"]\n","\n","  preprocessing_function = get_preprocessing_function(architecture)\n","  # # initialize preprocessor\n","  preprocessor = Preprocessor(\n","      input_directory=input_directory,\n","      rescale=rescale,\n","      shape=shape,\n","      color_mode=color_mode,\n","      preprocessing_function=preprocessing_function,\n","  )\n","\n","  tst_img_list = [glob.glob(args.tst_pth + \"/good/*.jpeg\"), glob.glob(args.tst_pth + \"/defect/*.jpeg\")]\n","  tst_lbl_list = [list (np.ones(len(tst_img_list[0]))), list (np.zeros(len(tst_img_list[1])))]\n","  tst_prd_list = [list (np.ones(len(tst_img_list[0]))), list (np.zeros(len(tst_img_list[1])))]\n","\n","  tst_img_list = tst_img_list[0] + tst_img_list[1]\n","  tst_lbl_list = tst_lbl_list[0] + tst_lbl_list[1]\n","  tst_prd_list = tst_prd_list[0] + tst_prd_list[1]\n","  \n","  for i,img_nm in enumerate(tst_img_list):\n","    img = preprocessor.get_test_image(img_nm)\n","    imgs_test_pred = model.predict(img)\n","    map = np.sqrt((imgs_test_pred - img)**2)\n","    # (score, diff ) = compare_ssim(imgs_test_pred.reshape(128,128)*255, img.reshape(128,128)*255, full=True)\n","    # map = map/np.max(map)\n","    kernel = np.ones((20, 20), np.uint8)\n","    map = cv2.morphologyEx(map[0], cv2.MORPH_CLOSE, kernel)\n","  #   print(\"--- %s seconds ---\" % (time.time() - start_time))\n","    map = np.where(map> th,1,0)\n","    # m_map = np.mean(map)\n","    # plt.imshow(map.reshape(128,128), cmap='gray')\n","    # plt.show()\n","    cnt_1 = np.count_nonzero(map)\n","    # print(cnt_1, f\" :\",tst_img_list[i])\n","    if cnt_1 > 1:\n","      tst_prd_list[i] = 0\n","    else:\n","      tst_prd_list[i] = 1\n","\n","# confusion matrix\n","  matrix = confusion_matrix(tst_lbl_list,tst_prd_list, labels=[1,0])\n","  print('Classification report : \\n',matrix)\n","  return matrix\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1k1HvJ0McZfN"},"source":["\n","def inference(args):\n","    # parse arguments\n","    model_path = args.path\n","    # ============= LOAD MODEL AND PREPROCESSING CONFIGURATION ================\n","    # load model and info\n","    model, info, _ = utils.load_model_HDF5(model_path)\n","    # set parameters\n","    input_directory = info[\"data\"][\"input_directory\"]\n","    architecture = info[\"model\"][\"architecture\"]\n","    loss = info[\"model\"][\"loss\"]\n","    rescale = info[\"preprocessing\"][\"rescale\"]\n","    shape = info[\"preprocessing\"][\"shape\"]\n","    color_mode = info[\"preprocessing\"][\"color_mode\"]\n","    vmin = info[\"preprocessing\"][\"vmin\"]\n","    vmax = info[\"preprocessing\"][\"vmax\"]\n","    nb_validation_images = info[\"data\"][\"nb_validation_images\"]\n","\n","    # =================== LOAD VALIDATION PARAMETERS =========================\n","\n","    # model_dir_name = os.path.basename(str(Path(model_path).parent))\n","    # finetune_dir = os.path.join(\n","    #     os.getcwd(),\n","    #     \"results\",\n","    #     input_directory,\n","    #     architecture,\n","    #     loss,\n","    #     model_dir_name,\n","    #     \"finetuning\",\n","    # )\n","    # subdirs = os.listdir(finetune_dir)\n","    # for subdir in subdirs:\n","    #     logger.info(\n","    #         \"testing with finetuning parameters from \\n{}...\".format(\n","    #             os.path.join(finetune_dir, subdir)\n","    #         )\n","    #     )\n","    #     try:\n","    #         with open(\n","    #             os.path.join(finetune_dir, subdir, \"finetuning_result.json\"), \"r\"\n","    #         ) as read_file:\n","    #             validation_result = json.load(read_file)\n","    #     except FileNotFoundError:\n","    #         logger.warning(\"run finetune.py before testing.\\nexiting script.\")\n","    #         sys.exit()\n","\n","   \n","\n","    #     # ====================== PREPROCESS TEST IMAGES ==========================\n","\n","    #     # get the correct preprocessing function\n","    preprocessing_function = get_preprocessing_function(architecture)\n","\n","    # # initialize preprocessor\n","    preprocessor = Preprocessor(\n","        input_directory=input_directory,\n","        rescale=rescale,\n","        shape=shape,\n","        color_mode=color_mode,\n","        preprocessing_function=preprocessing_function,\n","    )\n","\n","    # get test image\n","    img = preprocessor.get_test_image(args.img_pth)\n","    plt.imshow(img.reshape(512,512), cmap='gray')\n","    plt.show()\n","    #predict on test image\n","    start_time = time.time()\n","\n","    imgs_test_pred = model.predict(img)\n","    map = np.sqrt((imgs_test_pred - img)**2)\n","\n","    print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","    kernel = np.ones((20, 20), np.uint8)\n","\n","    map = cv2.morphologyEx(map[0], cv2.MORPH_CLOSE, kernel)\n","\n","    plt.imshow(imgs_test_pred.reshape(512,512), cmap='gray')\n","    plt.show()\n","\n","    \n","\n","    (score, diff ) = compare_ssim(imgs_test_pred.reshape(512,512)*255, img.reshape(512,512)*255, full=True)\n","    # diff = 1 - (diff/np.max(diff) ) \n","    # diff = (diff - np.mean(diff)) / np.std(diff)\n","    # print(np.max(diff))\n","    # diff = (diff * 255) \n","    \n","\n","    map = np.where(map> 0.4,1,0)\n","\n","    plt.imshow(map.reshape(512,512), cmap='gray')\n","    plt.show()\n","\n","    \n","    plt.imshow(diff, cmap = 'gray')\n","    plt.show()\n","\n","    #     # instantiate TensorImages object\n","    #     tensor_test = postprocessing.TensorImages(\n","    #         imgs_input=imgs_test_input,\n","    #         imgs_pred=imgs_test_pred,\n","    #         vmin=vmin,\n","    #         vmax=vmax,\n","    #         method=method,\n","    #         dtype=dtype,\n","    #         filenames=filenames,\n","    #     )\n","\n","    #     # ====================== CLASSIFICATION ==========================\n","\n","    #     # retrieve ground truth\n","    #     y_true = get_true_classes(filenames)\n","\n","    #     # predict classes on test images\n","    #     y_pred = predict_classes(\n","    #         resmaps=tensor_test.resmaps, min_area=min_area, threshold=threshold\n","    #     )\n","\n","    #     # confusion matrix\n","    #     tnr, fp, fn, tpr = confusion_matrix(y_true, y_pred, normalize=\"true\").ravel()\n","\n","    #     # initialize dictionary to store test results\n","    #     test_result = {\n","    #         \"min_area\": min_area,\n","    #         \"threshold\": threshold,\n","    #         \"TPR\": tpr,\n","    #         \"TNR\": tnr,\n","    #         \"score\": (tpr + tnr) / 2,\n","    #         \"method\": method,\n","    #         \"dtype\": dtype,\n","    #     }\n","\n","    #     # ====================== SAVE TEST RESULTS =========================\n","\n","    #     # create directory to save test results\n","    #     save_dir = os.path.join(\n","    #         os.getcwd(),\n","    #         \"results\",\n","    #         input_directory,\n","    #         architecture,\n","    #         loss,\n","    #         model_dir_name,\n","    #         \"test\",\n","    #         subdir,\n","    #     )\n","\n","    #     if not os.path.isdir(save_dir):\n","    #         os.makedirs(save_dir)\n","\n","    #     # save test result\n","    #     with open(os.path.join(save_dir, \"test_result.json\"), \"w\") as json_file:\n","    #         json.dump(test_result, json_file, indent=4, sort_keys=False)\n","\n","    #     # save classification of image files in a .txt file\n","    #     classification = {\n","    #         \"filenames\": filenames,\n","    #         \"predictions\": y_pred,\n","    #         \"truth\": y_true,\n","    #         \"accurate_predictions\": np.array(y_true) == np.array(y_pred),\n","    #     }\n","    #     df_clf = pd.DataFrame.from_dict(classification)\n","    #     with open(os.path.join(save_dir, \"classification.txt\"), \"w\") as f:\n","    #         f.write(\n","    #             \"min_area = {}, threshold = {}, method = {}, dtype = {}\\n\\n\".format(\n","    #                 min_area, threshold, method, dtype\n","    #             )\n","    #         )\n","    #         f.write(df_clf.to_string(header=True, index=True))\n","\n","    #     # print classification results to console\n","    #     with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n","    #         print(df_clf)\n","\n","    #     # save segmented resmaps\n","    #     #if save:\n","    #     save_segmented_images(tensor_test.resmaps, threshold, filenames, save_dir)\n","\n","    #     # print test_results to console\n","    #     print(\"test results: {}\".format(test_result))\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7XdDJ5p3kYM"},"source":["parser = argparse.ArgumentParser() \n","parser.add_argument(\"-p\",\n","    \"--path\", type=str, default= None ,required=True, metavar=\"\", help=\"path to saved model\"\n",")\n","parser.add_argument(\n","    \"-i\", \"--img_pth\", type=str, default = 'None', help=\"save segmented images\",\n",")\n","\n","parser.add_argument(\n","     \"--tst_pth\", type=str, default = None, help=\"path to test folder\",\n",")\n","\n","notebook_args = f\"\"\"\n","--path \"path/to/model\"\n","--img_pth \"path/to/single/images/inference\"\n","--tst_pth \"path/to/test/folder\"\n","\"\"\"\n","args = parser.parse_args(shlex.split(notebook_args))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTca0o1n4r-N"},"source":["matr = Test(args, 0.4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"BbRoap-0qJ0e","outputId":"c77a8440-18ac-40c3-fbe9-b4b97e9f2db8"},"source":["ax= plt.subplot()\n","sns.heatmap(matr, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","ax.set_title('Confusion Matrix'); \n","ax.xaxis.set_ticklabels(['good', 'defect']); ax.yaxis.set_ticklabels(['good', 'defect']);"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"LxzYlnQ62vIO"},"source":["inference(args)"],"execution_count":null,"outputs":[]}]}